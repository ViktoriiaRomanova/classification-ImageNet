{"cells":[{"cell_type":"markdown","metadata":{"id":"LcLIGgzBNeHS"},"source":["## Changes for Colaboratory"],"id":"LcLIGgzBNeHS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdJDeS68fvUs"},"outputs":[],"source":["# flake8-noqa-cell\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"OdJDeS68fvUs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIdD1Kskj-5k"},"outputs":[],"source":["import sys"],"id":"bIdD1Kskj-5k"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDbNd2BFNudT"},"outputs":[],"source":["sys.path.append('/content/drive/MyDrive/ImageNetproject')"],"id":"iDbNd2BFNudT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF3fbEQlNyAT"},"outputs":[],"source":["sys.path"],"id":"xF3fbEQlNyAT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWwKLUMPMSIS"},"outputs":[],"source":["path_to_drive = '/content/drive/MyDrive/ImageNetproject/'\n","\n","# Move dataset to current work directory\n","!cp -R $path_to_drive'ImageNetDataSet/unzipedPart/ILSVRC' '/content/'"],"id":"AWwKLUMPMSIS"},{"cell_type":"markdown","metadata":{"id":"1d1b3c21"},"source":["# Train VGG model"],"id":"1d1b3c21"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMtFK_-6fpl6"},"outputs":[],"source":["import pickle\n","import random\n","import shutil\n","from typing import Any, Tuple\n","\n","import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary\n","from torchvision import transforms\n","from tqdm import tqdm\n","\n","from processingDataSet import conv_to_img, ImageNetDataset\n","from VGGModel import vgg19"],"id":"cMtFK_-6fpl6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9928d836"},"outputs":[],"source":["!nvidia-smi"],"id":"9928d836"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5549736d"},"outputs":[],"source":["!python3 --version"],"id":"5549736d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"374839e0"},"outputs":[],"source":["data_path = '/content/ILSVRC/Data/CLS-LOC/train'"],"id":"374839e0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"07ffacc6"},"outputs":[],"source":["random_seed = 10\n","device = 'cuda'"],"id":"07ffacc6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd5a7d32"},"outputs":[],"source":["random.seed(random_seed)\n","torch.manual_seed(random_seed)"],"id":"dd5a7d32"},{"cell_type":"code","execution_count":null,"metadata":{"id":"587f62ee"},"outputs":[],"source":["transform_train = transforms.Compose([\n","    transforms.Resize([224, 224]),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","    transforms.RandomRotation(random.randint(0, 180)),\n","    transforms.RandomHorizontalFlip(p=0.2)])"],"id":"587f62ee"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b51e0ee7"},"outputs":[],"source":["transform_test = transforms.Compose([\n","    transforms.Resize([224, 224]),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"],"id":"b51e0ee7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4a238de"},"outputs":[],"source":["model = vgg19(num_classes = 10).to(device)"],"id":"e4a238de"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f5d84ef"},"outputs":[],"source":["summary(model, (3, 224, 224), device = device)"],"id":"9f5d84ef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"424a2eac"},"outputs":[],"source":["# Hyper parameters\n","lr = 0.0025\n","batch_size = 64\n","epochs = 30"],"id":"424a2eac"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c78c908"},"outputs":[],"source":["with open(path_to_drive + 'filtered_data_10class.pkl', 'rb') as file:\n","    train_data = pickle.load(file)\n","    val_data = pickle.load(file)"],"id":"4c78c908"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d75848c"},"outputs":[],"source":["train_set = ImageNetDataset(data_path, train_data, transform_train)\n","val_set = ImageNetDataset(data_path, val_data, transform_test)"],"id":"2d75848c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a27a47df"},"outputs":[],"source":["print('Train data size: ', len(train_set), 'Validation data size: ', len(val_set))"],"id":"a27a47df"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa75e6b2"},"outputs":[],"source":["train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, drop_last = True)\n","val_loader = DataLoader(val_set, batch_size = batch_size, shuffle = False, drop_last = True)"],"id":"fa75e6b2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"94ef1160"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max',\n","                                                       factor = 0.5, patience = 3,\n","                                                       cooldown = 5)\n","loss_func = nn.CrossEntropyLoss()"],"id":"94ef1160"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3a4c3f8"},"outputs":[],"source":["def train(model: nn.Module, opt: torch.optim.Optimizer, scheduler: Any,\n","          loss_fn: nn.Module, epochs: int, data_tr: DataLoader,\n","          data_val: DataLoader, writer: SummaryWriter, start: int = 0) -> None:\n","    \"\"\"\n","    Start train and validation mode for epochs amount.\n","\n","    Save:\n","    * metrics into TensorBoard at each epoch;\n","    * model and optimizer parameters every 5 epochs.\n","    \"\"\"\n","    for epoch in range(start, start + epochs):\n","        avg_loss = 0\n","        train_accuracy = 0\n","\n","        model.train()\n","        for x_batch, y_batch in tqdm(data_tr):\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","\n","            opt.zero_grad()\n","            y_pred = model(x_batch)\n","            loss = loss_fn(y_pred, y_batch)\n","            loss.backward()\n","            opt.step()\n","\n","            # Calculate average train loss and accuracy\n","            avg_loss += (loss/len(data_tr)).detach().cpu()\n","            # !it is not final result, to get real accuracy need to divide into num_batches\n","            train_accuracy += torch.sum(torch.argmax(y_pred, 1) == y_batch) / len(y_batch)\n","\n","            del x_batch, y_batch, y_pred, loss\n","\n","        train_accuracy /= len(data_tr)\n","\n","        avg_val_loss = 0\n","        val_accuracy = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for x, y in data_val:\n","                x, y = x.to(device), y.to(device)\n","                y_pred = model(x)\n","                loss = loss_fn(y_pred, y)\n","                avg_val_loss += (loss/len(data_val)).detach().cpu()\n","                val_accuracy += torch.sum(torch.argmax(y_pred, 1) == y) / len(y)\n","                del x, y, y_pred, loss\n","\n","        val_accuracy /= len(data_val)\n","\n","        writer.add_scalar('Lr', opt.state_dict()['param_groups'][0]['lr'], epoch)\n","        writer.add_scalars('Loss', {'train': avg_loss, 'val': avg_val_loss}, epoch)\n","        writer.add_scalars('Accuracy', {'train': train_accuracy, 'val': val_accuracy}, epoch)\n","        scheduler.step(val_accuracy)\n","\n","        print('Train_loss: ', avg_loss, 'Train_accuracy: ', train_accuracy, '\\n',\n","              'Val_loss: ', avg_val_loss, 'Val_accuracy: ', val_accuracy,\n","              'Lr: ', opt.state_dict()['param_groups'][0]['lr'])\n","\n","        if (epoch + 1) % 5 == 0:\n","            # Save model state on Google Drive\n","            torch.save({\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': opt.state_dict()},\n","                      path_to_drive + 'checkpoints1/modelOptE{0}.pt'.format(epoch))\n","\n","            # Copy TensorBoard logs to Google Drive\n","            shutil.copytree('runs', path_to_drive + 'logs1/runs{0}'.format(epoch), dirs_exist_ok = True)\n","\n","    writer.close()"],"id":"c3a4c3f8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbFnaQ9O3y01"},"outputs":[],"source":["# Loading model weights which we got before (if we have it)\n","checkpoints = torch.load(path_to_drive + 'checkpoints1/modelOptE29.pt')\n","model.load_state_dict(checkpoints['model_state_dict'])\n","optimizer.load_state_dict(checkpoints['optimizer_state_dict'])\n","\n","# Move TensorBoard log (if it exists) to the work directory\n","!cp -r $path_to_drive'logs1/runs29/' 'runs/'"],"id":"YbFnaQ9O3y01"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRaniymx6nN9"},"outputs":[],"source":["# Load TensorBoard  extention and start it\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"id":"cRaniymx6nN9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8e1b65e"},"outputs":[],"source":["start = 30  # amount of epochs which went before\n","writer = SummaryWriter('runs/expColab2', flush_secs = 1)\n","train(model, optimizer, scheduler, loss_func, epochs, train_loader, val_loader, writer, start = start)"],"id":"c8e1b65e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b23584be"},"outputs":[],"source":["# Save model state on Google Drive\n","torch.save({'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()},\n","           path_to_drive + 'checkpoints/modelOptFinal.pt')"],"id":"b23584be"},{"cell_type":"markdown","source":["## Look at the results"],"metadata":{"id":"grsRpM-hkG8R"},"id":"grsRpM-hkG8R"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vogH-k1gfhLi"},"outputs":[],"source":["def predict(model: nn.Module, loader: DataLoader) -> Tuple[torch.tensor, torch.tensor]:\n","    \"\"\"\n","        Predict class label.\n","\n","        Returns: Tuple[real, predicted]\n","    \"\"\"\n","    with torch.no_grad():\n","        logits = []\n","        real_label = []\n","\n","        for x, y in tqdm(loader):\n","            x = x.to(device)\n","            model.eval()\n","            y_pred = model(x).cpu()\n","            logits.append(y_pred)\n","            real_label.append(y)\n","            del x, y\n","\n","    pred = torch.argmax(torch.softmax(torch.cat(logits), dim=-1), 1)\n","    real = torch.cat(real_label)\n","    return real, pred"],"id":"vogH-k1gfhLi"},{"cell_type":"code","source":["f1_score(*predict(model, val_loader), average = 'micro')"],"metadata":{"id":"Z248-1FljRwX"},"id":"Z248-1FljRwX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(4, 4, figsize = (20, 20))\n","ind = random.sample(range(len(val_set)), 16)  # get 16 random indexes of pictures\n","\n","with torch.no_grad():\n","    model.eval()\n","    for i, fig_x in zip(ind, ax.flatten()):\n","        x, y = val_set[i]\n","        x = torch.unsqueeze(x, 0).to(device)\n","        y_pred_prob = torch.softmax(model(x), dim = -1).cpu()\n","        prob = torch.max(y_pred_prob).item() * 100\n","        y_pred = torch.argmax(y_pred_prob)\n","        label = 'Real class: {0}, Predicted class: {1},\\n Probability: {2:.0f}%'.format(y, y_pred, prob)\n","        fig_x.title.set_text(label)\n","        fig_x.imshow(conv_to_img(x))\n","plt.show()"],"metadata":{"id":"pwVdYfIqkZRJ"},"id":"pwVdYfIqkZRJ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6F-2ulJbg9SH"},"outputs":[],"source":["# Making flake8 checks\n","# !flake8-nb $path_to_drive'ColabWorkflow/trainVGGModel.ipynb'"],"id":"6F-2ulJbg9SH"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"notebookId":"a72f27d5-e99f-424c-8142-729707cc4f77","notebookPath":"trainVGGModel.ipynb"},"nbformat":4,"nbformat_minor":5}